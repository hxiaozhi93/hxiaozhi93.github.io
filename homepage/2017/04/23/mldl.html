<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="keywords" content="MLDL" />
    <meta name="description" content="MLDL" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0, user-scalable=0" />
    <title>小志同学 - MLDL</title>
    <link rel="shortcut icon" href="favicon.ico" />
    <link rel="stylesheet" href="https://cdn.bootcss.com/highlight.js/8.0/styles/sunburst.min.css" />
    <link rel="stylesheet/less" href="/homepage/assets/style/style.less" />
    <script src="https://lib.sinaapp.com/js/jquery/1.9.1/jquery-1.9.1.min.js"></script>
    <script src="https://cdn.bootcss.com/less.js/1.7.0/less.min.js"></script>
    <script src="https://cdn.bootcss.com/highlight.js/8.0/highlight.min.js"></script>
</head>
<body>
<script>
    var pageData = {};
    var _hmt = _hmt || [];
    (function () {
        var hm = document.createElement('script');
        hm.src = 'https://hm.baidu.com/hm.js?6b9830e6ab8073ce1a44ad49a03d8596';
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>
<img src="http://oij8a9ql4.bkt.clouddn.com/portrait.jpg" alt="小志同学" width="0" height="0" />
<nav>
    <a href="javascript:;" class="js_menu_btn">
        <span></span>
        <span></span>
        <span></span>
    </a>
    <ul class="js_menu">
        <li><a href="/homepage/">Home</a></li>
        <li><a href="/homepage/views/posts" class="js_goto_posts">Posts</a></li>
        <li><a href="/homepage/views/about">About</a></li>
    </ul>
</nav>
<a href="javascript:;" class="gotop js_gotop"></a>
<header class="post" style="background-image: url('http://oij8a9ql4.bkt.clouddn.com/banner.jpg')">
    <div>
        <h2>MLDL</h2>
        <time>23 Apr 2017</time>
        <p class="abstract">机器学习和深度学习相关的课程与资料</p>
        <ul class="c-fix">
            
            <li>机器学习</li>
            
            <li>深度学习</li>

            
        </ul>
    </div>
</header>
<article class="js_article">
 

<hr />

<p>李飞飞CS231n课程：Convolutional Neural Networks for Visual Recognition</p>
<p>吴恩达MOOC机器学习课程</p>
<p>UFLDL课程：http://deeplearning.stanford.edu/wiki/index.php/UFLDL教程</p>
<p>台大林轩田MOOC：机器学习基石</p>
<p>麦子学院课程：深度学习基础介绍（机器学习），深度学习进阶（算法与应用）</p>
<p>Peter Harrington《机器学习实战》</p>
<p>赵永科 《深度学习：21天实战Caffe》</p>
<p>郑泽宇 顾思宇 《Tensorflow：实战Google深度学习框架》</p>
<p>黄文坚 唐源 《TensorFlow实战》</p>
<p>北京理工大学-礼欣、嵩天 MOOC《Python机器学习应用》</p>

<hr />

<p>AI(e.g. knowledge bases)/machine learning(e.g. logistic regression)/representation learning(e.g. autoencoders)/deep learning(e.g. MLPs)</p>
<p>rule-based systems：input/hand-designed program/output</p>
<p>classic machine learning:input/hand-designed features/mapping from features/output</p>
<p>representation learning:input/features/mapping from features/output</p>
<p>deep learning:input/simplest features/most complex features/mapping from features/output</p>



<hr />

<p>小批量随机梯度下降？</p>
<p>因为在大规模数据的训练中，训练数据可达百万级，这样更新参数笨重，常用方法就是利用训练集中的一小部分数据来更新参数。实际情况中，数据集不会包含重复图像，小批量数据的梯度就是对整个数据集梯度的一个近似。因此，实践中通过小批量数据的梯度可以实现更快的收敛，从此来进行更频繁的参数更新。</p>
<p>前向传播阶段，数据源起于数据读取层，经过若干处理层，得到逐层输出，最后一层与目标函数比较得到损失函数。损失函数在前向传播中计算得到，同时也是反向传播的起点。误差函数衡量模型预测值与真实值之间的差距，差距越大，代价越大，从而对模型参数优化起到指引作用。误差函数表征真实值与模型预测值之间的差异，还要不断更改模型参数，使得差异越来越小，这个过程就是最优化，使用梯度下降法。</p>
<p>反向传播阶段，计算梯度的过程即为每层首先计算相对于该层输出节点的梯度，之后使用链式法则将误差梯度传递至输入节点。输出单元的误差梯度通过对损失函数求导得到。采用随机梯度下降的求解方法对模型优化，也即让损失函数达到全局最小。</p>
<p>总结一下，SGD方法包括输入少量样本，计算输出和误差，计算这些样本的平均梯度，根据梯度调节权值，对训练集中大量的小样本重复该过程，直到目标函数平均值停止下降或达到预定迭代次数。之所以被称为随机梯度下降，是由于每个小样本子集提供了所有样本平均梯度的带噪声估计。</p>


<hr />

<p>结构性特征表示：高层表达由底层表达的组合而成。</p>
<p>深度学习实质：通过构建具有多隐层的机器学习模型和海量的训练数据，来学习更有用的特征，从而获得最终用于分类或预测的准确性。</p>
<p>稀疏编码（Sparse coding）：将一个信号表示为一组基的线性组合，而且要求只需较少的n个基就可表示信号。稀疏性：只有很少的n个非零元素或只有很少的n个远大于零的元素。SC是一种无监督学习方法，它用来寻找一组超完备基向量来更高效的表示样本数据。超完备基的好处是他们能更有效地找出隐含在输入数据内部的结构和模式。</p>
<p>CNN训练前权值用不同的小随机数初始化：小随机数用来保证网络不会因权值过大而进入饱和状态从而导致训练失败，不同用来保证网络可能正常学习，相同数初始化权重矩阵则无能力学习。</p>

<hr />

<p>降维？</p>
<p>降维必要性：多重共线性，高维空间本身具有稀疏性，过多变量妨碍查找规律的建立，仅在变量层面分析可能会忽略变量之间的潜在联系。</p>
<p>降维目的：减少预测变量的个数，确保这些变量是相互独立的，提供一个框架来解释结果。</p>
<p>降维方法：主成分分析、因子分析、用户自定义复合。</p>
<p>PCA：对高维数据降维，经过降维去除噪声，发现数据中的模式。</p>
<p>PCA过程：矩阵特征中心化，再计算其协方差矩阵，再计算其特征值和特征向量，再选取大的特征值对应的特征向量，得到新的数据集。</p>
<p>白化（whitening）：在PCA的基础上再除以每一个特征的标准差，以使其normalization，标准差即为奇异值的平方根。</p>
<p>Bach Normalization：提高梯度传递性，可允许更高学习速率，降低对初始值的依赖性。</p>

<hr />

<p>轻松看懂机器学习十大常用算法:http://www.jianshu.com/p/55a67c12d3e9</p>
<p>1. 决策树:根据一些 feature 进行分类，每个节点提一个问题，通过判断，将数据分为两类，再继续提问。这些问题是根据已有数据学习出来的，再投入新数据的时候，就可以根据这棵树上的问题，将数据划分到合适的叶子上。</p>
<p>2. 随机森林:在源数据中随机选取数据，组成几个子集,这 M 个子集得到 M 个决策树,将新数据投入到这 M 个树中，得到 M 个分类结果，计数看预测成哪一类的数目最多，就将此类别作为最后的预测结果。</p>
<p>3. 逻辑回归：当预测目标是概率这样的，值域需要满足大于等于0，小于等于1的，这个时候单纯的线性模型是做不到的，因为在定义域不在某个范围之内时，值域也超出了规定区间。这个模型需要满足两个条件 大于等于0，小于等于1</p>
<p>4. SVM：要将两类分开，想要得到一个超平面，最优的超平面是到两类的 margin 达到最大，margin就是超平面与离它最近一点的距离。</p>
<p>5. 朴素贝叶斯：例原始问题是为给你一句话，它属于哪一类。通过 bayes rules 变成一个比较简单容易求得的问题</p>
<p>6. K最近邻：给一个新的数据时，离它最近的 k 个点中，哪个类别多，这个数据就属于哪一类</p>
<p>7. K均值：聚类是一个将数据集中在某些方面相似的数据成员进行分类组织的过程，聚类就是一种发现这种内在结构的技术，聚类技术经常被称为无监督学习。k均值聚类是最著名的划分聚类算法，由于简洁和效率使得他成为所有聚类算法中最广泛使用的。给定一个数据点集合和需要的聚类数目k，k由用户指定，k均值算法根据某个距离函数反复把数据分入k个聚类中。</p>
<p>8. Adaboost：bosting就是把若干个分类效果并不好的分类器综合起来考虑，会得到一个效果比较好的分类器。</p>
<p>9. 神经网络：</p>
<p>10. 马尔可夫：Markov Chains 由 state 和 transitions 组成。</p>

<hr />

<p>《机器学习基石》  台大  林轩田：</p>
<p>“感知机”，线性分类器（二元分类）</p>
<p>感知机学习算法（PLA），线性不可分的数据线： Pocket 算法</p>
<p>根据不同输出空间Y 的学习类型：二元分类，多元分类，回归分类，结构化学习</p>
<p>根据不同数据标签yn 的学习类型：监督式学习：全部yn，非监督式学习：没有yn，半监督式学习：部分yn，增强式学习：隐形yn 通过goodness(ŷ) 表示</p>
<p>不同协议（Protocol）的机器学习：批量（batch）：所有已知的数据，线上(online) ：顺序（被动）数据，主动（active）：有策略地观测数据</p>
<p>不同输入空间X 的学习分类：具体(Concrete)：复杂（及相关）的物理意义，原始(Raw)：简单的物理意义，抽象(Abstract)：无(或少)的物理意义</p>
<p>相对于N 个输入x1, x2…,xN 的最大种类的线/有效线的数量</p>
<p>成长函数：通过选择所有可能的(x1,x2,.., xN) 所对应的最大值来去除依赖</p>
<p>如果k 个输入不能被H打碎，我们称k 为H的一个拐点</p>
<p>上限函数 B(N, k)：当拐点为 k 时，成长函数的最大值</p>
<p>VC 维度的定义：最大非拐点（maxmium non-break point）的正式名称</p>
<p>线性回归：找到具有小残差（residuals）的线/超平面</p>
<p>Logistic 回归，Logistic 函数，交叉熵误差，梯度下降</p>
<p>SGD 算法，</p>
<p>多元逻辑回归分类：One-Versus-All (OVA) 分解，One-versus-one （OVO）分解</p>
<p>非线性变换：Z 空间的直线 ⇔X空间特殊的二次曲线，非线性Φ + 线性模型= 非线性模型</p>
<p>坏的泛化：低的Ein，高的Eout；过拟合：Ein 变低，Eout 变高</p>
<p>过拟合：使用过度的dVC，噪声，数据量N 有限</p>
<p>解决过拟合：从简单模型开始，数据清理/修剪，数据暗示，正则化（regularization），验证（validation）</p>



<hr />

<p>决策树：熵，信息获取量：gain（A）=info（D）-info_A（D），某个属性gain越大，则以其为节点。</p>
<p>决策树算法：ID3，C4.5，CART，相同点（都是贪心算法，自上而下），不同点（属性选择度量方法不同，分别为information gain，gain ratio，gini index）</p>


<hr />

<p>神经网络调整因素：</p>
<p>神经网络结构：层数，每层神经元个数</p>
<p>初始化w、b的方法</p>
<p>cost函数</p>
<p>regularization：L1，L2，parameter</p>
<p>激活函数：sigmoid，tanh，relu</p>
<p>使用dropout？</p>
<p>训练集大小</p>
<p>minibatch 大小</p>
<p>学习率</p>
<p>梯度下降：SGS，hessian优化，momentum-based GD</p>

<hr />


<p>《Python机器学习应用-MOOC-北京理工大学-礼欣、嵩天》</p>
<p>sklearn库的基本功能：分类，回归，聚类，降维，模型选择，数据预处理</p>
<p>分类：最近邻算法/neighbors.NearestNeighbors，支持向量机/svm.SVC，朴素贝叶斯/naive_bayes.GaussianNB，决策树/tree.DecisionTreeClassifier，集成方法/ensemble.BaggingClassifier，神经网络/neural_network.MLPClassifier.</p>
<p>回归：岭回归/linear_model.Ridge，Lasso回归/linear_model.Lasso，弹性网络/linear_model.ElasticNet，最小角回归/linear_model.Lars，贝叶斯回归/linear_model.BayesianRidge，逻辑回归/linear_model.LogisticRegression，多项式回归/preprocessing.PolynomialFeatures。</p>
<p>聚类：K-means/cluster.KMeans，AP聚类/cluster.AffinityPropagation，均值漂移，层次聚类，DBSCAN，BIRCH，谱聚类。</p>
<p>降维：主成分分析/decomposition.PCA，截断SVD和LSA，字典学习，因子分析，独立成分分析，非负矩阵分解，LDA。</p>
<p>数据集：如from sklearn.datasets import load_digits;digits=load_digits();print (digits.data.shape);print (digits.target.shape);print (digits.images.shape);import matplotlib.pyplot as plt;plt.matshow(digits.image[0]);plt.show();</p>
<p>聚类：根据数据的“相似性”将数据分为多类的过程。评估不同样本间的“相似性”，通常方法为计算两个样本间的“距离”。欧式距离，曼哈顿距离，马氏距离（表示数据的协方差距离，是一种尺度无关的度量方式），夹角余弦（用向量空间中两个向量夹角的余弦值作为衡量两个样本差异的大小），聚类算法所在模块：sklearn.cluster。</p>
<p>K-means聚类算法：k-means算法以k为参数，把n个对象分成k个簇，使簇内具有较高的相似度，而簇间的相似度较低。（随机选择k个点作为初始的聚类中心，对于剩下的点，根据其与聚类中心距离将归入最近簇, 对每个簇，计算所有点的均值作为新聚类中心,重复直到聚类中心不再发生改变</p>
<p>DBSCAN聚类算法：算法是一种基于密度的聚类：聚类的时候不需要预先指定簇个数,最终的簇个数不定,DBSCAN DBSCAN 算法将数据点分为三类：核心点：在半径 Eps 内含有超过 MinPts数目的点,边界点：在半径 Eps 内点的数量小于 MinPts ，但是落在核心点的邻域内,噪音点.</p>
<p>主成分析（ Principal Component Analysis ，PCA ）是最常用的 一种降维方法，通常用于高数据集的探索与可视化，还可以作据压缩和预处理等。原理：矩阵的主成分就是其协方差对应的特征向量，按照的特征值大小进行排序，最大的特征值就是第一主成分，其次是第二成分，以此类推。</p>
<p>非负矩阵分解（ Non -negative Matrix Factorization，NMF ） 是在矩阵中所有元素均为非负数约束条件之下的矩阵分解方法。基本思想：给定一个非负矩阵 V，NMF 能够找到一个非负矩阵 W和一个 非负矩阵 H，使得矩阵 W和H的乘积近似等于矩阵 V中的值。</p>
<p>精确率： 精确率是针对我们预测结果而言的 ，（ 以二分类为例 ）它 表示 的是预测为正样本中有多少真。那么就两种可能了，一种就是把正类 预测为正类(TP) ，另一种就是把负类 预测为正类(FP)，P=TP/(TP+FP）。召回率： 是针对我们原来的样本而言，它表示中正例有多 少被预测正确了。那也有两种可能，一是把原来的正类 预测为正类(TP) ， 另一种就是把原来的正类 预测为负类(FN) ，R=TP/(TP+FN）。准确率=(TP+TN)/(FN+FP+TN) .</p>
<p>朴素贝叶斯分类器是一个以贝叶斯定理为基础 的多分类的分类器。对于给定数据，首先基于特征的条件独立性假 设，学习输入输出的联合概率分布，然后基于此模型，对给定的输入 x，利用贝叶斯定理求出后验概 率最大的输出 y。</p>
<p>决策树 是一种树形结构的分类器，通过顺序 询问分类点的属性决定最终别。通常根据特征的信息增益或其他指标，构建一颗决策树。在分类时，只需要按照决策中的结点依次进行判断，即可得到样本所属类别。</p>
<p>强化学习就是程序或智能体（ agent ）通过与环境不断地进行交互学习一 个从环境到动作的映射，学习目标就是使累计 回报 最大化。强化学习是一种试错学习，因其在各状态（环境）下需要尽量尝所有可以选择的动作，通过环境给出反馈（即奖励）来判断动作的优劣，最终获得环境和优动作的映射关系（即策略）。</p>
<p>马尔可夫决策过程（ Markov Decision tree）通常用来描述一个强化学习问题
 智能体 agent 根据 当前对 环境的 观察 采取动作获得环境的 反馈 ，并使 环境 发生改变的循环过程。MDP基本元素：状态集合，动作集合，状态转移模型，agent采取某动作后的即时奖励，策略等。状态值函数：表示执行策略能得到的累计折扣奖励。状态动作值函数：表示在状态s下执行动作a能得到的累计折扣奖励。</p>
<P>在现实的强化学习任务中，环境的转移概率、奖励函数往很难得知，甚至很难得知环境中有多少状态。若学习算法不再依赖于环境建模，则称为 免模型学习 ，蒙特卡洛强化学习就是其中的一种。蒙特卡洛强化学习使用多次采样，然后求取平均累计奖赏作为期望累计奖赏的近似。</P>
<p>蒙特卡洛强化学习算法需要采样一个完整的轨迹来更新值函数，效率较低， 此外 该算法没有充分利用强化学习任务的序贯决策结构。
 Q-learning算法结合了动态规划与蒙特卡洛方的思想，使得学习更加高效。</p>
<p>传统强化学习：真实环境中的状态数目过多，求解困难。深度 强化 学习：将 深度学习和强化学习结合在 一起，通过深度神经网络 直接学习环境（或观察）与状态动作值函数 Q( s,a )之间的映射关系，简化问题的求解。
</p>


<hr />

<p>小白。勿念。</p>

<hr />

<p>作者：无名氏。</p>



</article>
<footer>
    <a href="/homepage/views/posts" class="js_goto_posts">返回目录 - 痕迹</a>
</footer>
<div class="browser js_browser">
    <img src="" />
</div>

<script src="/homepage/assets/script/main.js"></script>
</body>
</html>
